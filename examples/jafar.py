# -*- coding: utf-8 -*-
"""colab_demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/PaulCouairon/JAFAR/blob/main/colab_demo.ipynb

# Jack Any Feature at Any Resolution

> ⚠️ Change your collab runtime to T4 GPU before running this notebook

In this notebook we will use JAFAR to upsample many VFMs and backbones such as Dino, Dinov2, Dinov2-R, SigLIP, ViT, etc. And we will basically compare the PCA before and after upsampling.


--------------------------
Do not hesitate to check our work and if you want more details, please refer to:
- [github](https://github.com/PaulCouairon/JAFAR)
- [arxiv](https://arxiv.org/abs/2506.11136)
- [project page](https://jafar-upsampler.github.io/)
"""



# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/PaulCouairon/JAFAR
# %cd JAFAR

"""# Install Missing Dependencies"""

!pip install hydra-core

from pathlib import Path
import torch
import torchvision.transforms as T
from PIL import Image

from hydra.utils import instantiate

from utils.img import unnormalize
from utils.visualization import plot_feats
import torch
from hydra import compose, initialize
from hydra.core.global_hydra import GlobalHydra
from hydra.utils import instantiate
from IPython.display import clear_output

project_root = str(Path().absolute())
!ln -s /content/JAFAR/config /tmp/config

!mkdir -p output/jafar
!mkdir -p output/jafar/vit_base_patch16_224.dino
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_base_patch16_224.dino.pth -O output/jafar/vit_base_patch16_224.dino/model.pth
!mkdir -p output/jafar/vit_base_patch16_224
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_base_patch16_224.pth -O output/jafar/vit_base_patch16_224/model.pth
!mkdir -p output/jafar/vit_small_patch14_dinov2.lvd142m
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_small_patch14_dinov2.lvd142m.pth -O output/jafar/vit_small_patch14_dinov2.lvd142m/model.pth
!mkdir -p output/jafar/vit_base_patch14_dinov2.lvd142m
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_base_patch14_dinov2.lvd142m.pth -O output/jafar/vit_base_patch14_dinov2.lvd142m/model.pth
!mkdir -p output/jafar/vit_small_patch14_reg4_dinov2
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_small_patch14_reg4_dinov2.pth -O output/jafar/vit_small_patch14_reg4_dinov2/model.pth
!mkdir -p output/jafar/vit_base_patch16_clip_384
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_base_patch16_clip_384.pth -O output/jafar/vit_base_patch16_clip_384/model.pth
!mkdir -p output/jafar/vit_base_patch16_siglip_512.v2_webli
!wget https://github.com/PaulCouairon/JAFAR/releases/download/Weights/vit_base_patch16_siglip_512.v2_webli.pth -O output/jafar/vit_base_patch16_siglip_512.v2_webli/model.pth

clear_output()

"""# Select an image

By default we select our parrot image, but we provide also a funciton to download a picture from internet to have more diversity.
"""

import requests
from PIL import Image
import torch
from torchvision import transforms
from io import BytesIO

import requests
from PIL import Image
import torch
from torchvision import transforms
from io import BytesIO
import matplotlib.pyplot as plt

def load_model(backbone, project_root):
    overrides = ["val_dataloader.batch_size=1", f"project_root={project_root}", f"backbone.name={backbone}"]
    if not GlobalHydra.instance().is_initialized():
        initialize(config_path="config", version_base=None)
    cfg = compose(config_name="base", overrides=overrides)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    backbone = instantiate(cfg.backbone)
    backbone.to(device)

    model = instantiate(cfg.model)
    model.cuda()
    model.eval()

    model.load_state_dict(torch.load(f"./output/jafar/{backbone.name}/model.pth", weights_only=False)["jafar"])
    clear_output()
    return model, backbone

def run_model_demo(backbone_name, title):
    """Run a demonstration with a specific backbone model."""
    print(f"\n{title}")
    print("=" * len(title))

    model, backbone = load_model(backbone_name, project_root)
    with torch.inference_mode():
      with torch.autocast("cuda",  dtype=torch.bfloat16):
        lr_feats, _ = backbone(image_batch)
        hr_feats = model(image_batch, lr_feats, (img_size, img_size))
    unorm_img_batch = unnormalize(image_batch, mean, std)
    plot_feats(unorm_img_batch[0], lr_feats[0], hr_feats[0])
    torch.cuda.empty_cache()

def download_image(url, save_path=None):
    """Download an image from URL and return PIL Image object"""
    # Make sure URL is a string, not a tuple
    if isinstance(url, tuple):
        url = url[0]

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

    response = requests.get(url, headers=headers, timeout=10)
    response.raise_for_status()

    image = Image.open(BytesIO(response.content)).convert("RGB")

    if save_path:
        image.save(save_path)
        print(f"Image saved to: {save_path}")

    return image


# Example URLs for random images
image_url = "https://picsum.photos/400/600",  # Random image from Lorem Picsum

# Load an image
img_size = 448
image_path = "./asset/parrot.png"  # Replace with the actual path to your image
image = Image.open(image_path).convert("RGB")
# image = download_image(image_url, save_path="downloaded_image.jpg")


device="cuda"

# Transform the image to match the input requirements of the model
mean = torch.tensor([0.485, 0.456, 0.406], device=device)
std = torch.tensor([0.229, 0.224, 0.225], device=device)
transform = T.Compose(
    [
        T.Resize(img_size),
        T.CenterCrop(img_size),
        T.ToTensor(),  # Convert to tensor
        T.Normalize(mean=mean, std=std),  # Normalize
    ]
)
image_batch = transform(image).unsqueeze(0).to(device)  # Add batch dimension and move to device

"""# Dinov2"""

run_model_demo("vit_small_patch14_dinov2.lvd142m", "Dinov2")

"""# SigLIP2"""

run_model_demo("vit_base_patch16_siglip_512.v2_webli", "SigLIP2")

"""# Dino"""

run_model_demo("vit_base_patch16_224.dino", "Dino")

"""# Dinov2-R"""

run_model_demo("vit_small_patch14_reg4_dinov2", "Dinov2-Registers")

"""# ViT-B"""

run_model_demo("vit_base_patch16_224", "ViT-B")

"""# Upsample At Any Resolution

JAFAR allows to upsample your features at any resolution ! Check by yourself.
"""

print("\nAny Resolution Demonstration")
print("=" * 30)

backbone = "vit_small_patch14_dinov2.lvd142m"
model, backbone = load_model(backbone, project_root)
lr_feats, _ = backbone(image_batch)

for img_size in [64, 128, 256, 448, 512]:
    print(f"Upsampling to {img_size}x{img_size}")
    with torch.inference_mode():
      with torch.autocast("cuda",  dtype=torch.bfloat16):
        hr_feats = model(image_batch, lr_feats, (img_size, img_size))
        unorm_img_batch = unnormalize(image_batch, mean, std)
        plot_feats(unorm_img_batch[0], lr_feats[0], hr_feats[0])
    torch.cuda.empty_cache()

print("\nJAFAR demonstration completed!")