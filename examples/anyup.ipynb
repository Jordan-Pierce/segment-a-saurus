{
  "cells": [
    {
      "metadata": {
        "id": "5db6104d717e7237"
      },
      "cell_type": "markdown",
      "source": [
        "## Minimal AnyUp Demo\n",
        "\n",
        "DINOv2 ViT-S features -> AnyUp upsampling -> joint PCA visualization\n",
        "\n",
        "Dependencies: `pip install torch torchvision transformers pillow matplotlib requests`"
      ],
      "id": "5db6104d717e7237"
    },
    {
      "metadata": {
        "collapsed": true,
        "ExecuteTime": {
          "end_time": "2025-10-15T11:18:27.553221Z",
          "start_time": "2025-10-15T11:18:14.575817Z"
        },
        "id": "initial_id"
      },
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from transformers import AutoImageProcessor, Dinov2Model\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 1) Download a sample image and force 448x448\n",
        "url = \"https://coralnet-production.s3.us-west-2.amazonaws.com/media/images/vs5rdpt3c7.jpg?AWSAccessKeyId=AKIAYVKEQ3B4DIOYONO3&Signature=frcOFhFJ25YtADeUzC%2Bz6WCqL%2BM%3D&Expires=1763084643\"\n",
        "img = Image.open(io.BytesIO(requests.get(url, timeout=10).content)).convert(\"RGB\")"
      ],
      "id": "initial_id",
      "outputs": [],
      "execution_count": 67
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) DINOv2 ViT-S/14 features (tokens at 32x32 for 448 with patch 14)\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-small\", use_fast=True, token=False)\n",
        "model = Dinov2Model.from_pretrained(\"facebook/dinov2-small\", token=False).to(device).eval()\n",
        "\n",
        "upsampler = torch.hub.load(\"wimmerth/anyup\", \"anyup\", verbose=False).to(device).eval()"
      ],
      "metadata": {
        "id": "8AIraKu10O7d"
      },
      "id": "8AIraKu10O7d",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small', use_fast=True, token=False)\n",
        "model = AutoModel.from_pretrained('facebook/dinov2-small', token=False).to(device).eval()"
      ],
      "metadata": {
        "id": "FuBWevJixmDB"
      },
      "id": "FuBWevJixmDB",
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "img_size = 1024\n",
        "\n",
        "inputs = processor(\n",
        "    images=img,\n",
        "    do_resize=True,\n",
        "    size={\"shortest_edge\": img_size},\n",
        "    do_center_crop=True,\n",
        "    crop_size={\"height\": img_size, \"width\": img_size},\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "hr_image = inputs[\"pixel_values\"].to(device)  # (1, 3, 448, 448), ImageNet-normalized\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model(pixel_values=hr_image)\n",
        "    tokens = out.last_hidden_state[:, 1:, :]  # drop [CLS]\n",
        "\n",
        "print(time.time() - t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYPaZSFAvA7q",
        "outputId": "7c30abb0-a436-4eb9-9eac-fca471580222"
      },
      "id": "PYPaZSFAvA7q",
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07887005805969238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "t0 = time.time()\n",
        "\n",
        "B, N, C = tokens.shape\n",
        "h = w = int(N ** 0.5)\n",
        "assert h * w == N, f\"Non-square token grid (N={N})\"\n",
        "lr_features = tokens.reshape(B, h, w, C).permute(0, 3, 1, 2).contiguous()  # (1, C, 32, 32)\n",
        "\n",
        "# 3) Load AnyUp from torch.hub and upsample to 448x448\n",
        "#    (hr_image must be ImageNet-normalized; lr_features can be from any encoder)\n",
        "\n",
        "with torch.no_grad():\n",
        "    # q_chunk_size can reduce memory if needed\n",
        "    hr_features = upsampler(hr_image, lr_features, q_chunk_size=256)  # (1, C, 448, 448)\n",
        "\n",
        "# 4) Joint PCA\n",
        "with torch.no_grad():\n",
        "    lr_flat = lr_features[0].permute(1, 2, 0).reshape(-1, C)\n",
        "    hr_flat = hr_features[0].permute(1, 2, 0).reshape(-1, C)\n",
        "    all_feats = torch.cat([lr_flat, hr_flat], dim=0)\n",
        "\n",
        "    mean = all_feats.mean(dim=0, keepdim=True)\n",
        "    X = all_feats - mean\n",
        "\n",
        "    U, S, Vh = torch.linalg.svd(X, full_matrices=False)\n",
        "    pcs = Vh[:3].T\n",
        "\n",
        "    proj_all = X @ pcs\n",
        "\n",
        "    # Split back and min-max normalize jointly for consistent coloring\n",
        "    n_lr = h * w\n",
        "    proj_lr = proj_all[:n_lr].reshape(h, w, 3)\n",
        "    proj_hr = proj_all[n_lr:].reshape(img_size, img_size, 3)\n",
        "\n",
        "    cmin = proj_all.min(dim=0).values\n",
        "    cmax = proj_all.max(dim=0).values\n",
        "    crng = (cmax - cmin).clamp(min=1e-6)\n",
        "\n",
        "    lr_rgb = ((proj_lr - cmin) / crng).cpu().numpy()\n",
        "    hr_rgb = ((proj_hr - cmin) / crng).cpu().numpy()\n",
        "\n",
        "print(time.time() - t0)"
      ],
      "metadata": {
        "id": "fhAiE6ppunwX"
      },
      "id": "fhAiE6ppunwX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5) Plot\n",
        "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
        "axs[0].imshow(lr_rgb)\n",
        "axs[0].set_title(f\"LR DINOv2 tokens ({h}×{w})\")\n",
        "axs[0].axis(\"off\")\n",
        "axs[1].imshow(hr_rgb)\n",
        "axs[1].set_title(f\"AnyUp upsampled ({img_size}×{img_size})\")\n",
        "axs[1].axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SKF552u5to2W"
      },
      "id": "SKF552u5to2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.array(hr_rgb).shape"
      ],
      "metadata": {
        "id": "cDThyvFtwYvA",
        "outputId": "d27ec197-e256-4870-a366-7181f264d655",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cDThyvFtwYvA",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(448, 448, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2lCVmPDXzzCW"
      },
      "id": "2lCVmPDXzzCW",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}